
\chapter{Background and Related Work}

\section{Type Systems}

%Exploring sophisticated type systems and their seamless integration into programming languages is a thoroughly researched field. From System $\FMu$  up to System $\FMuOmega$ , how far can we go until these systems are no longer suitable for compilers.

Type systems are a fundamental tool in programming language theory, providing a framework to categorize and constrain the behavior of programs, while types define the kind of data a program manipulates, e.g., integers, booleans, strings. 

Type systems manage a program's data types through type checking, which can be classified as either static---occurs at compile time, where the types of all variables and expressions are known and checked before the program runs---or dynamic---occurs at run-time, where the types are checked as the program executes. 

Over the course of time, type systems have evolved to become more expressive, moving from simple type assignments to more powerful systems that support polymorphism, recursion, and structured communication patterns.

System $F$, also called the polymorphic lambda calculus, extends the simply typed lambda calculus with \textit{parametric polymorphism}, which allows functions to be written generically and applied to arguments of any type. [ref] In the simply typed lambda calculus, every function has a concrete type. For instance, a function that operates on integers has the type $Int \xrightarrow{} Int$. However, in order to express a function that operates on a lists of arbitrary types, we would need a separate function for each of the types in the list. To overcome this limitation, System $F$ allowed functions to be polymorphic. For example, the identity function, written $\lambda X. \lambda x:X. x$, returns its argument unchanged, working uniformly for any type---when instantiated to $\Int$, the result is the identity function on integers, when applied to $Bool$, the result is the identity function on booleans. This is possible through the introduction of type \textit{abstractions}---functions are parameterized by types---and type \textit{applications}---types are passed as arguments to functions.

While System $F$ is a powerful system for expressing parametric polymorphism, it lacks a direct way to express recursive types, which are essential for defining more complex structures and functions that rely on self-reference. Recursive types are particularly important for defining data structures like lists, trees, and infinitely repeating processes.

System $\FMu$ comes as an extension of System $F$ with the addition of \textit{recursion}, denoted by $\mu X.T$, which allows a type $\TT$ to refer to itself through the variable $X$. \\

%\todo{Simple example of a list of integers}

By allowing recursion at the type level, System $\FMu$ significantly increases the expressiveness of the type system, making it capable of representing potentially infinite structures. However, now care must be taken to ensure that recursive functions terminate, which leads to type systems often incorporating additional rules or constraints to ensure \textit{termination}. Without guarantees of termination, recursive types could lead to non-terminating computations, i.e., infinite recursions, which can make programs unreliable.  

%\todo{note about grammar, pushdown automata, corresponding to the system.}

System $\FMuSemi$ builds on top of System $\FMu$ by incorporating \textit{session types}into the type system. This extension is designed to model structured communication between concurrent processes, ensuring that communication follows a well-defined protocol. 

\section{Session Types}

Session types offer a formal way to describe communication protocols. They ensure that all participants in a communication process adhere to the agreed protocol, improving both safety and expressiveness. This concept emerged in the early 1990s from the fields of process calculi and type theory, particularly building upon Milner's work on the pi-calculus.[ref] Its main purpose was to bring type-level guarantees to communication protocols.

In a programming language, \textit{types} describe the structure of data (e.g., integers, strings) and session types extend this idea to describe communication patterns. They define the order and type of messages exchanged during an interaction, through a communication channel. 
For example, a process might want to send an integer, then receive an integer and finally terminate. This protocol can be described by sending, $!\TT$, and receiving, $?\TT$, operations. In $!\Int.?\Int.\End$, we first send an integer, followed by receiving an integer and finally terminate the session through the type $\End$---represents closing a communication channel. The order in which the messages (types) are sent is guaranteed through a sequential composition expressed by the $.$ operator.

Communication typically involves two parties---a sender and a receiver---and for a protocol to function correctly, the session types of these parties must be dual. \textit{Duality} is the relationship between two session types that allows them to correctly engage in a protocol. For any session type describing one side of the communication (let us say the client), there is a dual session type that describes the other side (the server). For example, the session type $\OUTn\Int$ describes sending an integer and its dual session type is to receive an integer, $\INn\Int$.  

Some, more advanced, communication patterns involve branching, where a process might choose, from a set of choices, how to continue the communication. 

$\intchoice\rchannel{isEven}{!Int.?Bool.End}{isZero}{!Int.?Bool.End}$

The previous type represents a protocol (from the client perspective) that offers two operations on integers: it checks whether an integer is even or odd, and it checks whether an integer is zero. Session types are able to express such protocols by offering choice operations---either internal, $\&$, or external, $+$---to the client, which in turn selects how to proceed with the communication: if the client wants to check if an integer is even, it must select the branch $isEven$.

One of the key advantages of session types is that they enable \textit{type-safe communication}. This means that at compile time, the system can check whether the communication patterns follow the prescribed protocol, preventing errors like sending a message of a wrong type. Session types can also help ensure \textit{deadlock freedom}, meaning the system avoids situations where two parties wait indefinitely for each other to send or receive a message. This is particularly important in distributed and concurrent systems.

Session types also have limitations---they are tail recursive---not being able to describe expressive types that refer to themselves at any given point of a sequential composition rather than at the end. For instance, the type \todo{example showing tail recursion.}, represents ... until the session terminates. \todo{show we cannot have recursion at any given point of the sequencial composition.} The lack of expressiveness with the increasing complexity of communication protocols in modern systems lead us to context-free session types.

%\todo{mention fmu and fmu. can be represented by finite-state automata somewhere.}
\section{Context-free Session Types}

Context-free session types break free from tail recursion by introducing the sequential composition operator $\Semi$ and the type $\Skip$---neutral element of sequential composition---which represents a protocol with no actions. The recursive type $\tmuinfix{\alpha}{\skind}{\extchoice\rchannel{\leafl}{\Skip}{\nodel}{\alpha\Semi\ \INp\Int\Semi\ \alpha}}\Semi\End_{\typec{?}}$ describes a protocol for safely streaming integer trees on a channel. The channel presents an external choice $\extchoice$ with two labels: if the continuation of the channel is $\leafl$, then no communication occurs but the channel is still open for further composition whereas, if the continuation of the channel is $\nodel$, then we have a left subtree, followed by an integer and a right subtree. When the whole tree is received, the channel is closed. 

It is also important to distinguish type $\End_{\typec{?}}$ from $\Skip$---the former represents the closure of a channel, $\typec{?}$ standing for either $\Wait$ or its' dual $\Close$, where no further communication is possible, while the latter allows continuing the communication where its' dual is himself.

Context-free session types are more expressive than their regular counterparts, covering a wider range of communication protocols and being representable by \textit{context-free grammars}.
Grammars can be specified by tuples of the form $(\mathcal{T, N}, X, \mathcal{R})$, where: 
\begin{enumerate}
    \item $\mathcal{T}$ is a finite set of terminal symbols such as $a, b, c$; 
    \item $\mathcal{N}$ is a finite set of non-terminal symbols, $X, Y, Z$;
    \item $X$ is the starting symbol. $X$ must be an element of $\mathcal{N}$, $X\in\mathcal{N}$;
    \item $\mathcal{R}$ is a finite set of productions, where $P \subseteq N \times (T \cup N)^*$. 
\end{enumerate}
A production rule in $\mathcal{R}$ is written as $X \rightarrow \sigma$: the left side of the arrow must be a non-terminal, $X\in\mathcal{N}$, while the right side must be a word, such that $\sigma\in(\mathcal{N}\cup\mathcal{T})^*$. Also, $\sigma$ can be the empty word.

We are particularly interested in simple grammars in Greibach normal form [ref]. Grammars are in \textit{Greibach normal form} if production rules take form as $X \rightarrow a\gamma$, where $X\in\mathcal{N}$, $a\in\mathcal{T}$ and $\gamma\in\mathcal{N}^*$. Grammars in GNF are \emph{simple} when for every non-terminal and terminal symbol there is at most one production $X \rightarrow a\gamma$ [ref].

\todo{conclude to freeST.}

\section{FreeST}
FreeST [ref], a concurrent functional programming language based on system $F^{\mu;}$, is regulated by context-free session types.

\todo{note about equirecursion somewhere, recursive type and its unfold are semantically equivalent.}

We use FreeST's syntax freely for the following example.

\begin{lstlisting}
type Stream a = &{Done: Skip, 
                  More: ?a; Stream a}
\end{lstlisting}

Type \lstinline|Stream| offers two choices: \lstinline|Done| which terminates the communication and \lstinline|More|, that reads a value \lstinline|?a| and iterates.
This type can be used as a parameter for a fold channel type.

\begin{lstlisting}
type Fold a b =  ?(b -> a -> b); ?b;
                    Stream a; !b; Close
\end{lstlisting}
\vspace{3mm}

The type \lstinline|Fold| first receives a folding function followed by the starting element and the elements to fold. We denote the elements we want to fold as a stream, using type \lstinline|Stream|. Then, we send the output of the fold as \lstinline|!b| and close the channel.

Let us look at a function that consumes a \lstinline|Fold| channel.

\begin{lstlisting}
foldServer : foralla.forallb. Fold a b -> Unit
foldServer c = 
    let (f,c) = receive c in 
    let (e,c) = receive c in 
    let (r,c) = foldS @a @b f e c in
    c |> send r |> close
               
foldS : foralla.forallb. (b -> a -> b) -> b 
            -> Stream a; !b; End 
            -> (b, !b;Close)
foldS f e (Done c) = (e,c)
foldS f e (More c) = 
    let (x, c) = receive c in
    foldS @a @b f (f e x) c
\end{lstlisting}
\vspace{3mm}
The first function, \lstinline|foldServer|, consumes the first two arguments of the type \lstinline|Fold| channel, that is, receives the folding function and the starting elements, sends the result and terminates the communication by closing the channel. \lstinline|foldS| consumes the stream of elements to fold until the branch \lstinline|Done| is selected. Then, we send the pair \lstinline|(e,c)|, \lstinline|e| being the output and \lstinline|c| the continuation channel.
Note that \lstinline{x |> f} stands for the inverse function application, that is \lstinline{x |> f |> g = g (f x)}.

Now we need a client for \lstinline|foldServer|. 

\begin{lstlisting}
type TreeC a = &{Leaf: Skip, 
                 Node: TreeC a;?a;TreeC a}
\end{lstlisting}
\vspace{3mm}

The type \lstinline|TreeC| offers two choices: \lstinline|Leaf| which terminates the communication and \lstinline|Node|, that receives the left side sub-tree, the root element of that node as \lstinline|?a| and the right side sub-tree.
A channel that receives trees in a serialized format would have the following type \lstinline|TreeChannel a = TreeC a;Close|.

Next we want to transform our trees from channels to streams. We can write a \lstinline|flatten| function that receives a \lstinline|TreeChannel| and a \lstinline|Stream| and outputs the unused part of the stream.

\begin{lstlisting}
flatten: foralla.forallc. TreeChannel a ->
                (Dual Stream a);c -> c
\end{lstlisting}
\vspace{3mm}

We can finally write our client. This client receives a \lstinline|TreeChannel|, the dual of a \lstinline|Fold| channel and outputs \textbf{\lstinline|True|}, if all the elements in the tree channel are non negative, or false otherwise.

\begin{lstlisting}
allPositive: TreeChannel Int ->
             Dual (Fold Int Bool) 1-> Bool
allPositive t c = 
    let c = send (lambdax:Bool.lambday:Int. x 
                    && y > 0 ) c in
    let c = send True c in
    let c = flatten @Int @?Bool;Wait t c in
    let (x,c) = receive c in
    wait c; x
\end{lstlisting}
\vspace{3mm}

First the client sends the folding function, $\lambda x\colon \textbf{\lstinline|Bool|}.\lambda y\colon\textbf{\lstinline|Int|}.\\ 
x\ \&\&\ y\textgreater 0 $, followed by the starting element  \textbf{\lstinline|True|}. Then we call function \lstinline|flatten|. Next, the client receives the output of the \lstinline|Fold| channel, consumes it and waits for the channel to close, $wait c;\ x$.
Note that syntax $flatten\ @Int\ @?Bool;Wait$ stands for term-level type application. 

We can then run both our server and client, like so:
\begin{lstlisting}
main : Bool
main = 
    let tr = forkWith @(TreeC Int) @() 
                      produce in
    let fw = forkWith @Dual (Fold Int Bool) 
                      @() foldServer in
    allPositive tr fw
\end{lstlisting}



\section{Type equivalence algorithms}
Algorithms for deciding the equivalence of types are intricately tied to the computational power and expressiveness of the underlying type system: the more expressive the type system is, the more sophisticated the algorithm will be.

%\todo{note type checking in system f is undecidable.}


\begin{figure}[h]
  \begin{align*}
    \typec T \grmeq & \function TT
    %\grmor \tbananas{l_i}{T_i} 
    % \grmor \Unit 
    \grmor  \foralltinfix\alpha\kind T 
    \grmor \typec{\alpha}
    \grmor \tmuinfix{\alpha}{\kind}{T} 
  \end{align*}
\end{figure}

The syntax above inherites from System $F$ variable names $\typec\alpha$, type quantification 
$\foralltinfix\alpha\kind T$ and functions $\function TU$. With the addition of recursion $\tmuinfix{\alpha}{\kind}{T}$, we arrive at System $\FMu$. %In said system, two types are equivalent if they are unfolded versions of each other. For example, the type $\mu\alpha.\alpha$ is equivalent to $\alpha(\mu\alpha.\alpha)$.[ref] 


\begin{figure}[h]
  \begin{align*}
  \typec T \grmeq & (\FMu)
    \grmor \tabs\alpha\kind T
    \grmor \tapp TT 
  \end{align*}
\end{figure}

With the introduction of type abstractions $\tabs\alpha\kind T$ and applications $\tapp TT $, we arrive at a higher-order version, denoted with $\omega$, of System $\FMu$. System $\FMu$'s computational power is akin to finite-state automata. [ref] However, if we raise such system to a higher-order version of itself, its' computational power would be at least as expressive as deterministic pushdown automata for which known equivalence algorithms are
notoriously impractical.  [ref]

%Building upon System $\FMu$, introducing (regular) session types with send/receive operations, choices, and $\End$ type, we arrive at System $\FMuDot$. 

\begin{figure}[h]
  \begin{align*}
    \typec T \grmeq & (\FMu)
    \grmor{} \MSGn T
    \grmor{} \tchoice{l_i}{T_i}
    \grmor \End
    \grmor{} \semit TT 
    \grmor \Skip
  \end{align*}
\end{figure}

Building upon System $\FMu$, introducing context-free session types with sequential composition and type $\Skip$, we arrive at System $\FMuSemi$. Two session types are considered equivalent if they describe exactly the same communication protocol, even if they are not identical syntactically. Thiemann and Vasconcelos proved that type equivalence of context-free session types is decidable [ref] by reducing the problem to the verification of bisimulation for Basic Process Algebra (BPA). It is also known the proof of equivalence between grammars in GNF and BPA processes. With these results,  Almeida et al. [ref] decides the equivalence of context-free session types by reducing the problem to the bisimilarity of simple grammars. The algorithm is divided in three phases:
\begin{itemize}
    \item Translating types into simple grammars;
    \item Pruning unreachable symbols;
    \item Exploring a decision tree: two types are equivalent if during this phase we are able to find an empty node; otherwise,
    the types are not equivalent.
\end{itemize}

An higher-order version of this system, $\FMuSemiOmega$, would still be as expressive as deterministic pushdown automata. However, Poças et al. [ref] proved that, for a small subset of the language where recursion is limited to kind $*$, type equivalence is still representable by simple grammars. Combining this notorious notion and Almeida's algorithm to check if two types are equivalent, we are able to elevate FreeST's type system to a higher-order setting.

%\todo{figure: systems and their computational models.}

%\input{tex/fig-systems.tex}

\section{Programming languages with type operators}

A type system is described as higher-order if it supports \textit{type-level functions}, i.e., if it acts upon types and produce even more complex types. Such systems are often realized through mechanisms such as type classes in Haskell or implicits in Scala.[ref]

\lstinline{data List a = Empty | Cons a (List a)}

This Haskell code represents a \lstinline{List}---a type constructor that takes a type \lstinline{a} and returns a new type \lstinline{List a}---which represents a list of elements of type \lstinline{a}. Therefore, the type \lstinline{List} operates on types, that means, when type \lstinline{a} is replaced by another type (let us say $\Int$) the constructor returns \lstinline{List Int}. While type constructors take (concrete) types as inputs, higher-order kinded types operate one level further. They are type operators which take type constructors themselves as inputs. 

%The kind of a type can be either a $\skind$, refering to a concrete type such as $\Int$ or $String$, a kind * -> * which represents a type constructor like a $List$ or the monad $Maybe$. 

A \textit{functor}, a type class where types can be mapped over, would be an example of a higher-order kinded type.

\lstinline{class Functor f where fmap :: (a -> b) -> f a -> f b}

In this example, the method \lstinline{fmap} takes two arguments, a function and a functor type \lstinline{f a}, and returns a new type \lstinline{f b} from mapping the first argument to each value in \lstinline{f a}.

Type classes, in Haskell, enable polymorphism by defining a set of operations that can be implemented for various types. A \textit{type class} is a higher-order abstraction that operates on types rather than values.

\lstinline{class Eq a where (==) :: a -> a -> Bool}

The example above represents \lstinline{Eq}, a type class that defines equality for any type \lstinline{a}. Type classes emerged from the need to deal with ad-hoc polymorphism, also known as overloading, providing a flexible structure to manage polymorphic operations. 
\textit{Ad-hoc polymorphism} refers to the ability of a function or operator to behave differently based on the type of its arguments, that means a function can have different implementations based on the types of its parameters and the compiler will choose the correct implementation based on the types of arguments passed.
While, \textit{Parametric polymorphism} refers to the ability of a function or a data type to be written generically so that it can handle values of any type, without type-specific implementations. For instance, in Haskell, the identity function can be written as, \lstinline{id :: a -> a id x = x}, where given any input \lstinline{x}, returns the same type. This function works generically as it does not matter which type \lstinline{x} has.

%\todo{comparison between oCaml and Haskell.}

A common ad-hoc polymorphism example would be addition through the $+$ operator. In oCaml, the $+$ operator does not allow overloading, that is, it only works to perform the addition of integer values. To perform addition of floating-point numbers a seperate operator must be used---the $+.$ operator.

In contrast, Haskell chooses to offer overloading of its' $+$ operator through the use of type classes. This operator works for both integer and floating-point addition or any type that is an instance of the $Num$ type class.

Scala’s implicits notion have a similar function to haskell's type classes, where the compiler ensures the correctness of these operations based on resolving these implicit types. The code provided below is the equivalent type class \lstinline{Eq} implemented in Scala: \todo{scala code.}

%\todo{Benefits of higher-order systems and type operators.}

%\todo{mention the higher-order version of the sytems are as expressive as pushdown automata.}

%\todo{mention limitation of myu kind possibly}

\LIMPA